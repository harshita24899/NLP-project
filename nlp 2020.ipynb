{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.in/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=headphones noise cancellation\n"
     ]
    }
   ],
   "source": [
    "userInput='headphones noise cancellation'\n",
    "url_amzn = 'https://www.amazon.in/s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=' + userInput\n",
    "print(url_amzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36'}\n",
    "source_code = requests.get(url_amzn, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_aps_sr_pg1_1?ie=UTF8&adId=A0903865M9H4LPEJ3Q8R&url=%2FEartrons-Over-Ear-Cancellation-Bluetooth-Headphones%2Fdp%2FB07WHLFL21%2Fref%3Dsr_1_1_sspa%3Fdchild%3D1%26keywords%3Dheadphones%2Bnoise%2Bcancellation%26qid%3D1601835974%26sr%3D8-1-spons%26psc%3D1&qualifier=1601835974&id=7497501057405939&widgetName=sp_atf\n"
     ]
    }
   ],
   "source": [
    "plain_text = source_code.text\n",
    "soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "links=[]\n",
    "for titles in soup.find_all('a', {'class': 'a-link-normal a-text-normal'},{'class':'a-size-medium a-color-base a-text-normal'}):\n",
    "            try:\n",
    "                links.append(titles)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "                \n",
    "a=(links[0])\n",
    "requiredLink='http://www.amazon.in'+a.get('href')\n",
    "print(requiredLink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Producing .json file associated with scraped data\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import ssl\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Extraction of data is complete. Check json file.----------\n"
     ]
    }
   ],
   "source": [
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "html = urllib.request.urlopen(requiredLink, context=ctx).read()\n",
    "html= requests.get(url_amzn, headers=headers)\n",
    "soup = BeautifulSoup(html.text)\n",
    "html = soup.prettify('utf-8')\n",
    "product_json = {}\n",
    "\n",
    "# This block of code will help extract the <Brand> of the item\n",
    "\n",
    "for divs in soup.findAll('div', attrs={'class': 'a-box-group'}):\n",
    "    try:\n",
    "        product_json['brand'] = divs['data-brand']\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# This block of code will help extract the <Product Title> of the item\n",
    "\n",
    "for spans in soup.findAll('span', attrs={'id': 'productTitle'}):\n",
    "    name_of_product = spans.text.strip()\n",
    "    product_json['name'] = name_of_product\n",
    "    break\n",
    "\n",
    "# This block of code will help extract the <price> of the item \n",
    "\n",
    "for divs in soup.findAll('div'):\n",
    "    try:\n",
    "        price = str(divs['data-asin-price'])\n",
    "        product_json['price'] = 'Rs.' + price\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# This block of code will help extract the <average star rating> of the product\n",
    "\n",
    "for i_tags in soup.findAll('i',\n",
    "                           attrs={'data-hook': 'average-star-rating'}):\n",
    "    for spans in i_tags.findAll('span', attrs={'class': 'a-icon-alt'}):\n",
    "        product_json['star-rating'] = spans.text.strip()\n",
    "        break\n",
    "\n",
    "# This block of code will help extract the <number of customer reviews> of the product\n",
    "\n",
    "for spans in soup.findAll('span', attrs={'id': 'acrCustomerReviewText'\n",
    "                          }):\n",
    "    if spans.text:\n",
    "        review_count = spans.text.strip()\n",
    "        product_json['customer-reviews-count'] = review_count\n",
    "        break\n",
    "\n",
    "# This block of code will help extract the <details> of the product\n",
    "\n",
    "product_json['details'] = []\n",
    "for ul_tags in soup.findAll('ul',\n",
    "                            attrs={'class': 'a-unordered-list a-vertical a-spacing-none'\n",
    "                            }):\n",
    "    for li_tags in ul_tags.findAll('li'):\n",
    "        for spans in li_tags.findAll('span',\n",
    "                attrs={'class': 'a-list-item'}, text=True,\n",
    "                recursive=False):\n",
    "            product_json['details'].append(spans.text.strip())\n",
    "\n",
    "#Extract <date of reviews>\n",
    "product_json['reviewed-date'] = []\n",
    "for divs in soup.findAll('span',\n",
    "                           attrs={'class': 'a-size-base a-color-secondary review-date'\n",
    "                           }):\n",
    "    reviewed_date = divs.text.strip()\n",
    "    product_json['reviewed-date'].append(reviewed_date)\n",
    "\n",
    "# This block of code will help extract the <long reviews> of the product\n",
    "\n",
    "product_json['long-reviews'] = []\n",
    "for divs in soup.findAll('div', attrs={'data-hook': 'review-collapsed'\n",
    "                         }):\n",
    "    long_review = divs.text.strip()\n",
    "    product_json['long-reviews'].append(long_review)\n",
    "\n",
    "# Saving the scraped html file\n",
    "\n",
    "with open('output_file.html', 'wb') as file:\n",
    "    file.write(html)\n",
    "\n",
    "# Saving the scraped data in json format\n",
    "\n",
    "with open('product.json', 'w') as outfile:\n",
    "    json.dump(product_json, outfile, indent=4)\n",
    "print ('----------Extraction of data is complete. Check json file.----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_reviews = pd.read_csv(\"amazon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Review_body</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "      <td>overall the quality music is awesome never had...</td>\n",
       "      <td>Reviewed in India on 14 August 2020</td>\n",
       "      <td>NOT THE BEST BLUETOOTH MODE HEADPHONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0 out of 5 stars</td>\n",
       "      <td>1. Anything you listen, even at 50% volume, is...</td>\n",
       "      <td>Reviewed in India on 21 August 2020</td>\n",
       "      <td>stay away!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "      <td>\\n  Noise cancellation doesn't work in this pr...</td>\n",
       "      <td>Reviewed in India on 19 August 2020</td>\n",
       "      <td>Quality is not good , NO noise cancellation fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>I have heard many headphones, but i choose ver...</td>\n",
       "      <td>Reviewed in India on 27 July 2020</td>\n",
       "      <td>Great and value for money product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>Battery Backup:\\n10/10\\nSound quality:\\n10/10\\...</td>\n",
       "      <td>Reviewed in India on 24 September 2020</td>\n",
       "      <td>Best value for money !!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Ratings                                        Review_body  \\\n",
       "0  3.0 out of 5 stars  overall the quality music is awesome never had...   \n",
       "1  2.0 out of 5 stars  1. Anything you listen, even at 50% volume, is...   \n",
       "2  1.0 out of 5 stars  \\n  Noise cancellation doesn't work in this pr...   \n",
       "3  5.0 out of 5 stars  I have heard many headphones, but i choose ver...   \n",
       "4  5.0 out of 5 stars  Battery Backup:\\n10/10\\nSound quality:\\n10/10\\...   \n",
       "\n",
       "                                     Date  \\\n",
       "0     Reviewed in India on 14 August 2020   \n",
       "1     Reviewed in India on 21 August 2020   \n",
       "2     Reviewed in India on 19 August 2020   \n",
       "3       Reviewed in India on 27 July 2020   \n",
       "4  Reviewed in India on 24 September 2020   \n",
       "\n",
       "                                         Review_head  \n",
       "0              NOT THE BEST BLUETOOTH MODE HEADPHONE  \n",
       "1                                        stay away!!  \n",
       "2  Quality is not good , NO noise cancellation fe...  \n",
       "3                 Great and value for money product.  \n",
       "4                            Best value for money !!  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Ratings', 'Review_body', 'Date', 'Review_head'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_reviews.shape\n",
    "(34660, 21)\n",
    "\n",
    "amz_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = amz_reviews[ 'Review_body']\n",
    "#columns=list(columns)\n",
    "#print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.375, -1.25, -1.75, 1.375, 3.125, 1.875, 3.75, 3.125, 1.25, 1.5, 0.0, 4.0, 2.875, 3.75, 2.0, 1.0, 1.625, 2.75, 2.5, -0.625, 3.125, 0.5, 0.5, 0.0, 1.125, 2.5, 2.375, -0.25, 0.5, 0.75, 0.125, -0.25, -0.625, 1.875, 0.75, 0.0, -0.25, 1.125]\n",
      "-----------------\n",
      "['neg', 'neg', 'neg', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neu', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'pos', 'pos', 'pos', 'neu', 'pos', 'pos', 'pos', 'neu', 'pos', 'pos', 'neu', 'neu', 'neg', 'pos', 'pos', 'neu', 'neu', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis without using the built-in tool\n",
    "\n",
    "'''\n",
    "1.Remove numbers, characters and white spaces from reviews\n",
    "2.If review has 'not' replace 'not' and the next word together with an Antonym\n",
    "3.Find the sentiScore for each word after tokenization\n",
    "4.Using above find the sentiScore of the sentence\n",
    "\n",
    "'''\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word):\n",
    "        ant = list()\n",
    "        for syn in wn.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms():\n",
    "                    ant.append(lemma.antonyms()[0].name())\n",
    "        if len(ant) >= 1:\n",
    "            return ant[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def negreplace(self, string):\n",
    "        i = 0\n",
    "        sent = word_tokenize(string)\n",
    "        len_sent = len(sent)\n",
    "        words = []\n",
    "        while i < len_sent:\n",
    "            word = sent[i]\n",
    "            if word == 'not' and i + 1 < len_sent:\n",
    "                ant = self.replace(sent[i + 1])\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "        return words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \n",
    "    #Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def swn_polarity(text):\n",
    "    \n",
    "    #Return a sentiment polarity: 0 = negative, 1 = positive\n",
    "    \n",
    "\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    b = \"\"\n",
    "    if \"not\" in text:\n",
    "        x = \"\"\n",
    "        raw_sentences = sent_tokenize(text)\n",
    "        for raw_sentence in raw_sentences:\n",
    "            tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "\n",
    "            for word, tag in tagged_sentence:\n",
    "                wn_tag = penn_to_wn(tag)\n",
    "                if wn_tag in (wn.NOUN, wn.ADJ):\n",
    "                    x = x + \" \" + word\n",
    "                elif word == 'not':\n",
    "                    x = x + \" \" + word\n",
    "                else:\n",
    "                    continue\n",
    "        rep = AntonymReplacer()\n",
    "        a = rep.negreplace(x)\n",
    "        for i in range(len(a)):\n",
    "            b = b + \" \" + str(a[i])\n",
    "    if b == \"\":\n",
    "        b = text\n",
    "    \n",
    "    raw_sentences = sent_tokenize(b)\n",
    "    for raw_sentence in raw_sentences:\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "\n",
    "        for word, tag in tagged_sentence: \n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "\n",
    "            # Take the first sense, the most common\n",
    "            \n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "    return sentiment\n",
    "\n",
    "score=[]\n",
    "reviews_list=amz_reviews['Review_body']\n",
    "for j in range(0,len(reviews_list)):\n",
    "    paragraph =reviews_list[j] \n",
    "\n",
    "    paragraph = re.sub(r'\\[[0-9]*\\]', ' ', paragraph)  #removing references to a particular line denoted as for e.g[3]\n",
    "    paragraph = re.sub(r'\\s+', ' ', paragraph)  #removing duplicate white spaces\n",
    "    paragraph\n",
    "\n",
    "    formatted_paragraph = re.sub('[^a-zA-Z]', ' ', paragraph )  #removing all characters except a-z and A-Z\n",
    "    formatted_paragraph = re.sub(r'\\s+', ' ', formatted_paragraph)  #removing duplicate white spaces\n",
    "    formatted_paragraph\n",
    "    num = swn_polarity(paragraph)\n",
    "    score.append(num)\n",
    "    #print(num)\n",
    "\n",
    "print(score)\n",
    "print('-----------------')\n",
    "    \n",
    "'''\n",
    "Clearly Positive*\t\"score\": 0.8,\n",
    "Clearly Negative*\t\"score\": -0.6, \n",
    "Neutral\t\"score\": 0.1, \n",
    "Mixed\t\"score\": 0.0, \n",
    "'''\n",
    "value=[]\n",
    "for i in score:\n",
    "    if i >= 0.5:\n",
    "        value.append('pos')\n",
    "    elif (i > -0.5) and (i < 0.5):\n",
    "        value.append( 'neu')\n",
    "    else:#score <= -0.5:\n",
    "        value.append ('neg')\n",
    "print(value)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'neg', 'neu', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neu', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neu', 'pos', 'pos', 'neu', 'neu', 'pos', 'pos', 'pos', 'pos', 'neu', 'neu', 'neu', 'pos', 'neu', 'pos', 'neu', 'neu', 'pos', 'neu']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# VADER sentiment analysis tool for getting Compound score.\n",
    "def sentimental(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    score=vs['compound']\n",
    "    return score\n",
    "\n",
    "\n",
    "# VADER sentiment analysis tool for getting pos, neg and neu.\n",
    "def sentimental_Score(sentence):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    score=vs['compound']\n",
    "    if score >= 0.5:\n",
    "        return 'pos'\n",
    "    elif (score > -0.5) and (score < 0.5):\n",
    "        return 'neu'\n",
    "    elif score <= -0.5:\n",
    "        return 'neg'\n",
    "\n",
    "\n",
    "    \n",
    "#reviews_list.apply(lambda x: sentimental_Score(x))\n",
    "res=list(map(sentimental_Score,reviews_list))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayy this product is for u only, pandaga chesko ;)\n"
     ]
    }
   ],
   "source": [
    "pos=value.count('pos')\n",
    "neg=value.count('neg')\n",
    "if pos>neg:\n",
    "    print('Ayy this product is for u only, pandaga chesko ;)')\n",
    "else:\n",
    "    print('Sry boss, this product aint for you :(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
